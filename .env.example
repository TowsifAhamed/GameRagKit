# ==============================================================================
# CLOUD PROVIDER CONFIGURATION
# ==============================================================================
# Choose your cloud provider: openai | gemini | azure | mistral | hf
# Get API keys:
#   - Gemini: https://makersuite.google.com/app/apikey
#   - OpenAI: https://platform.openai.com/api-keys
#   - Azure: Azure Portal → OpenAI Resource → Keys and Endpoint
#   - Mistral: https://console.mistral.ai/
#   - HuggingFace: https://huggingface.co/settings/tokens

PROVIDER=openai
API_KEY=
ENDPOINT=

# Example configurations for different providers:
# --- Gemini (2025 models) ---
# PROVIDER=gemini
# API_KEY=AIzaSy...
# ENDPOINT=https://generativelanguage.googleapis.com/
# CLOUD_CHAT_MODEL=gemini-2.0-flash-exp
# CLOUD_EMBED_MODEL=text-embedding-004
#
# --- OpenAI (2025 models) ---
# PROVIDER=openai
# API_KEY=sk-proj-...
# ENDPOINT=https://api.openai.com/
# CLOUD_CHAT_MODEL=gpt-4.1
# CLOUD_EMBED_MODEL=text-embedding-3-large
#
# --- Azure OpenAI ---
# PROVIDER=azure
# API_KEY=your-azure-key
# ENDPOINT=https://your-resource.openai.azure.com/

# ==============================================================================
# LOCAL PROVIDER CONFIGURATION (Optional - for offline/hybrid mode)
# ==============================================================================
LOCAL_ENGINE=ollama
OLLAMA_HOST=http://host.docker.internal:11434
LOCAL_CHAT_MODEL=llama3.2:3b-instruct-q4_K_M
LOCAL_EMBED_MODEL=nomic-embed-text
# After starting Ollama: ollama pull llama3.2:3b-instruct-q4_K_M

# ==============================================================================
# MODEL OVERRIDES (Optional - specify exact models per provider)
# ==============================================================================
CLOUD_CHAT_MODEL=gpt-4.1
CLOUD_EMBED_MODEL=text-embedding-3-large
# Latest models by provider (2025):
#   Gemini: gemini-2.0-flash-exp, gemini-2.5-flash, gemini-2.5-pro / text-embedding-004
#   OpenAI: gpt-4.1, gpt-4.1-mini, gpt-4o / text-embedding-3-large, text-embedding-3-small
#   Mistral: mistral-large-latest / mistral-embed

# ==============================================================================
# VECTOR DATABASE CONFIGURATION (Required for RAG)
# ==============================================================================
# Supported: pgvector | qdrant
DB=pgvector
CONNECTION_STRING=Host=postgres;Port=5432;Username=postgres;Password=postgres;Database=gamerag
# If using docker-compose, the above default works out of the box

# Alternative: Qdrant
# DB=qdrant
QDRANT_ENDPOINT=http://qdrant:6333
QDRANT_COLLECTION=gamerag

# ==============================================================================
# HTTP API CONFIGURATION (Optional - only needed if running as API server)
# ==============================================================================
CORS_ORIGINS=
# Comma-separated list of allowed origins, e.g.: http://localhost:3000,https://yourgame.com
